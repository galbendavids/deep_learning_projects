{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galbendavids/deep_learning_projects/blob/main/Nureal_network_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTWuud84G-MB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i2mf1yoHC0D",
        "outputId": "457079fc-a332-4d78-890d-af9535d0b122"
      },
      "source": [
        "def initialize_parameters(layer_dims):#need to check with shay\n",
        "  W = []\n",
        "  b = []\n",
        "  for layer in range(1, len(layer_dims)):\n",
        "    #Return a sample (or samples) from the “standard normal” distribution.\n",
        "    b.append(np.zeros(layer_dims[layer]))\n",
        "    #shays code\n",
        "    W.append(np.random.randn(layer_dims[layer],layer_dims[layer-1]))\n",
        "    ################\n",
        "    #NEW TRY, i think we need to transpose the weights\n",
        "    #W.append((np.random.randn(layer_dims[layer],layer_dims[layer-1])).T)\n",
        "    ################\n",
        "  return np.array(W),np.array(b)\n",
        "print(initialize_parameters([2,3,1])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.3863887 ,  0.03868427],\n",
            "       [-0.25262791,  0.66689991],\n",
            "       [-0.08579473,  0.58145371]])\n",
            " array([[-0.47192173,  0.5673913 ,  0.11664484]])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4dNB2-THEjp",
        "outputId": "44492cda-7acf-4568-f1d8-c63fe2b0b557"
      },
      "source": [
        "def linear_forward(A, W, b): \n",
        "  Z = W.dot(A) + b\n",
        "  return Z, {'linear_cache': [A, W, b]} # fix return a dic not array\n",
        "a = np.array([[ 5, 1 ,3], \n",
        "              [ 1, 1 ,1], \n",
        "              [ 1, 2 ,1]])\n",
        "b = np.array([1, 2, 3])\n",
        "linear_forward(b, a, np.ones(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([17.,  7.,  9.]), {'linear_cache': [array([1, 2, 3]), array([[5, 1, 3],\n",
              "          [1, 1, 1],\n",
              "          [1, 2, 1]]), array([1., 1., 1.])]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3_JtAA0Hr9h"
      },
      "source": [
        "def softmax(Z):\n",
        "  return np.exp(Z) / np.sum(np.exp(Z)), {'activation_cache': [Z]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBgReML-MC75",
        "outputId": "b8d56e68-f88e-4dcd-8f43-73c068b3fb88"
      },
      "source": [
        "import copy\n",
        "def relu(Z):\n",
        "  Z_relu = copy.deepcopy(Z) # check if must\n",
        "  Z_relu[Z<0]=0\n",
        "  return Z_relu, {'activation_cache': [Z]}\n",
        "\n",
        "x = np.random.randn(2,2)\n",
        "print(relu(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[0.        , 1.41480948],\n",
            "       [1.67232025, 0.        ]]), {'activation_cache': [array([[-1.36652612,  1.41480948],\n",
            "       [ 1.67232025, -0.47674891]])]})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K8-QLTfOJG-",
        "outputId": "c32f5a96-f2bb-4f41-df6c-3110090fc180"
      },
      "source": [
        "def linear_activation_forward(A_prev, W, B, activation): #need to check\n",
        "  Z, linear_cache = linear_forward(A_prev, W, B)\n",
        "  if activation == \"relu\":\n",
        "    output, activation_cache = relu(Z)\n",
        "    linear_cache.update(activation_cache)\n",
        "    return output, linear_cache\n",
        "  else: # softmax\n",
        "    output, activation_cache = softmax(Z)\n",
        "    linear_cache.update(activation_cache)\n",
        "    return output, linear_cache\n",
        "linear_activation_forward(np.random.randn(5,4), initialize_parameters([5,5])[0], initialize_parameters([5,4])[1], \"x\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[5.69239886e-03, 8.44855094e-02, 2.53759232e-03, 4.09446166e-03],\n",
              "         [5.92295574e-01, 8.79306681e-02, 6.18184453e-03, 4.14218053e-03],\n",
              "         [6.30725853e-03, 2.24067524e-02, 2.80270502e-02, 8.63415740e-03],\n",
              "         [3.11861133e-02, 3.17372366e-02, 1.23087722e-02, 5.59012098e-03],\n",
              "         [3.76904006e-04, 5.63717808e-02, 6.14315277e-03, 3.55047125e-03]]]),\n",
              " {'activation_cache': [array([[[-1.08213288,  1.6153154 , -1.89004891, -1.41162939],\n",
              "           [ 3.56274116,  1.65528401, -0.99964794, -1.40004229],\n",
              "           [-0.97956352,  0.28809773,  0.51190549, -0.66553851],\n",
              "           [ 0.61870827,  0.63622601, -0.31095244, -1.10026371],\n",
              "           [-3.79702939,  1.21070406, -1.00592654, -1.55418429]]])],\n",
              "  'linear_cache': [array([[ 0.29544011, -1.25253613,  0.61781314,  1.23621274],\n",
              "          [-0.92074263, -0.71017019,  0.20463074,  0.54179585],\n",
              "          [-2.34367611,  0.181712  , -1.02903811, -0.65666005],\n",
              "          [ 2.06059389,  0.04124191,  0.11360048,  0.08596826],\n",
              "          [ 0.63672448,  0.13082405, -0.33613189,  0.22139208]]),\n",
              "   array([[[-0.97650297, -0.0903064 ,  0.73627737, -0.05203995,\n",
              "             1.50150534],\n",
              "           [-0.59779217, -0.97751988,  0.54101797,  1.91001486,\n",
              "             0.26937031],\n",
              "           [-0.46657186,  0.04920489, -0.34477153, -0.34267612,\n",
              "            -1.41086287],\n",
              "           [ 0.39906073, -1.61017467,  0.74821214,  0.79022742,\n",
              "            -1.34519346],\n",
              "           [-0.91745132, -0.11794309,  0.30753859, -1.36234605,\n",
              "            -0.16735637]]]),\n",
              "   array([[0., 0., 0., 0.]])]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0nTAYJPdc3y",
        "outputId": "479fc6d7-ee81-4435-bc14-d2138c249e5d"
      },
      "source": [
        "#1f\n",
        "def L_model_forward(X, parameters, use_batchnorm): #need to check with shay\n",
        "\n",
        "\n",
        "  ##for section 3 and 4\n",
        "  if(use_batchnorm):\n",
        "    #do something\n",
        "    print(use_batchnorm)\n",
        "  ##\n",
        "\n",
        "  cache={}\n",
        "  W,B = parameters\n",
        "  L=parameters[0].shape[0]+1 #num of layers\n",
        "  \n",
        "  A=X #installation. is that true?\n",
        "\n",
        "  for i in range(0,L-2): # in L layer net there are L-2 feeding forwards\n",
        "\n",
        "    \n",
        "    #printing for debuging, delete after shays checks it\n",
        "    print(\"iteration:\"+ str(i))\n",
        "    print(\"A\")\n",
        "    print(X)\n",
        "    print(X.shape)\n",
        "    print(\"W\")\n",
        "    print(W[i].T)\n",
        "    print(W[i].T.shape)\n",
        "    print(\"B\")\n",
        "    print(B[i])\n",
        "    print(B[i].shape)\n",
        "    #printing for debugging --- > end\n",
        "\n",
        "    Z, linear_cache = linear_forward( W[i].T,A, B[i].T)\n",
        "    cache.update(linear_cache)\n",
        "    A, activation_cache = relu(Z)\n",
        "    cache.update(activation_cache)\n",
        "  \n",
        "  Z, linear_cache = linear_forward(W[L-2].T,A, B[L-2].T)\n",
        "  cache.update(linear_cache)\n",
        "  AL, activation_cache = softmax(Z)\n",
        "  cache.update(activation_cache)\n",
        "  return AL, cache\n",
        "\n",
        "\n",
        "input_size=3\n",
        "num_of_examples=5\n",
        "X=np.random.randn(num_of_examples,input_size)\n",
        "parameters=initialize_parameters([input_size,3,3,3,1])\n",
        "use_batchnorm=False #should we apple bactchnorm after activatin - \n",
        "                    #note that this option needs to be set to “false” in Section 3 and “true” in Section 4\n",
        "\n",
        "AL_,cache_=(L_model_forward(X,parameters,False))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration:0\n",
            "A\n",
            "[[ 1.19665584  0.76827184 -0.07305142]\n",
            " [-0.79789926 -0.79493072  0.13863908]\n",
            " [ 0.80394854  0.09121762  0.24100775]\n",
            " [-1.37567168 -0.15665453  0.12848972]\n",
            " [-0.53707113 -0.03480119  0.08074453]]\n",
            "(5, 3)\n",
            "W\n",
            "[[ 1.88617078  0.58190462  0.01052073]\n",
            " [-1.2032277   1.49277819  0.7156293 ]\n",
            " [-3.04612755  0.79899008  0.36959002]]\n",
            "(3, 3)\n",
            "B\n",
            "[0. 0. 0.]\n",
            "(3,)\n",
            "iteration:1\n",
            "A\n",
            "[[ 1.19665584  0.76827184 -0.07305142]\n",
            " [-0.79789926 -0.79493072  0.13863908]\n",
            " [ 0.80394854  0.09121762  0.24100775]\n",
            " [-1.37567168 -0.15665453  0.12848972]\n",
            " [-0.53707113 -0.03480119  0.08074453]]\n",
            "(5, 3)\n",
            "W\n",
            "[[-0.47631961  0.57867524 -0.03363722]\n",
            " [ 1.16347718 -1.71294858 -0.0253761 ]\n",
            " [-0.20437755 -0.32438223  0.01625974]]\n",
            "(3, 3)\n",
            "B\n",
            "[0. 0. 0.]\n",
            "(3,)\n",
            "iteration:2\n",
            "A\n",
            "[[ 1.19665584  0.76827184 -0.07305142]\n",
            " [-0.79789926 -0.79493072  0.13863908]\n",
            " [ 0.80394854  0.09121762  0.24100775]\n",
            " [-1.37567168 -0.15665453  0.12848972]\n",
            " [-0.53707113 -0.03480119  0.08074453]]\n",
            "(5, 3)\n",
            "W\n",
            "[[-1.13603346  0.73428284  0.82489614]\n",
            " [-1.1271307  -1.91179577 -1.6487858 ]\n",
            " [ 1.33852654 -1.10978127 -0.18792362]]\n",
            "(3, 3)\n",
            "B\n",
            "[0. 0. 0.]\n",
            "(3,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x67MoSlfossE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97658a8-8ba6-47ed-f633-0d16f3dcb463"
      },
      "source": [
        "#1g\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):  #need to check with shay\n",
        "  m=AL.shape[1]\n",
        "  cost_matrix = (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1-AL).T))\n",
        "  cost_value=(-1/m)*sum(sum(cost_matrix))\n",
        "  return cost_value\n",
        "\n",
        "#trial\n",
        "compute_cost(AL_,AL_*2)# not real values.. only for the example.."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.966153274936996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Xas3uqZPyH"
      },
      "source": [
        "#2 \n",
        "def\tlinear_backward(dZ, cache):\n",
        "  A_prev, W, b_prev = cache['linear_cache']\n",
        "  dW = dZ.dot(A_prev.T) # sould do Transpse?\n",
        "  db = dZ.dot(np.ones(np.shape(dZ)[1]))\n",
        "  dA_prev = W.T.dot(dZ)\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "#a = np.array([[ 1 ,3], \n",
        "#              [ 1 ,1], \n",
        "#              [ 2 ,1]])\n",
        "#b = np.array([[1], [2]])\n",
        "linear_backward(np.random.randn(3,1), linear_forward(b, a, np.ones(3))[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5-SuH5NuPFV"
      },
      "source": [
        "def\trelu_backward(dA, activation_cache):\n",
        "  Z = activation_cache[\"activation_cache\"][0] # need to check\n",
        "  dZ = copy.deepcopy(dA)\n",
        "  dZ[Z<0] = 0 \n",
        "  return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1j4_LZEw31z"
      },
      "source": [
        "def softmax_backward(dA, activation_cache):\n",
        "  Z = activation_cache[\"activation_cache\"][0]\n",
        "  sig = 1 / (1 + math.exp(-x))\n",
        "  dZ = dA * sig(Z) * (1 - sig(Z))\n",
        "  return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwsdqIvNzKz7"
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "  if activiation == \"relu\":\n",
        "    dZ = relu_backward(dA, cache)\n",
        "  else:\n",
        "    dZ = softmax_backward(dA, cache)\n",
        "  dA_prev, dW, db = linear_backward(dZ, cache)\n",
        "  return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCAaHwlE7fFC"
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "  #dA_priv sould be clac\n",
        "  dA_prob = None\n",
        "  Grads = {}\n",
        "  counter = len(caches)\n",
        "  for cache in caches[::-1]:\n",
        "    dA_prev, dW, db = linear_activation_backward(dA_prob, cache, \"relu\")\n",
        "    Grands[\"dA\"+str(counter)] = dA_prev\n",
        "    Grands[\"dW\"+str(counter)] = dW\n",
        "    Grands[\"db\"+str(counter)] = db\n",
        "    dA_prob = copy.deepcopy(dA_prev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnTYeddCT00d"
      },
      "source": [
        "def Update_parameters(parameters, grads, learning_rate):\n",
        "  \n",
        "  return parameters "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0kutouQWb56"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxrultHuX2Ht"
      },
      "source": [
        "while True:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}